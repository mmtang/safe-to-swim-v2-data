{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d8e9f9-caab-481b-baa6-16ef7d941c1f",
   "metadata": {},
   "source": [
    "# Processing FIB data for the Safe to Swim map (v2)\n",
    "\n",
    "### Introduction\n",
    "The following code processes fecal indicator bacteria data (FIB) for the Safe to Swim map (v2), which is currently in development. It sources FIB data from the [BeachWatch](https://beachwatch.waterboards.ca.gov/) and [California Environmental Data Exchange Network (CEDEN)](https://ceden.org/) databases, both of which are managed by the [State Water Resources Control Board](https://www.waterboards.ca.gov/). It combines the two datasets and calculates the rolling 30-day and 6-week geometric mean values for each data point. The FIB data used in this script includes sampling data for *E. coli*, Enterococcus, Fecal Coliform, and Total Coliform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2245d-eb72-4ef5-9a6e-18dd96c9f114",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "To run the following code, you will need Python 3.x installed along with the Python packages, pandas and pyodbc. You will also need access to the internal BeachWatch and CEDEN data tables via internal data mart or some other access point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9414c-0d05-422c-bdfe-b95810b42348",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "Run the following code cells in sequential order. You can run them manually cell by cell or run them all in one go. Do not skip any steps or cells. Depending on your computer and/or internet connection, it can take around two hours to run the script in its entirety. The generated data files are saved in the main directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee19775-e36d-43dc-8a54-0ba76f12d5dc",
   "metadata": {},
   "source": [
    "### 1. Import the required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907ceba-9ca9-4d6d-8fd6-76a44de1a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc # Used for connecting to the internal data marts\n",
    "from scipy.stats.mstats import gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d2af1-38cf-4204-b2e7-bb489854baf3",
   "metadata": {},
   "source": [
    "### 2. Download FIB data from BeachWatch and CEDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60027d-0730-4d2b-9ef3-695bd00efb57",
   "metadata": {},
   "source": [
    "#### 2.1 BeachWatch\n",
    "Define the variables for connecting to BeachWatch. These are private login credentials. The code block below will not run unless the environment variables on your machine are set up similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ab3c1-46fc-482b-89a6-e2e9e7fd2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "BW_SERVER1 = os.environ.get('S2S_Server')\n",
    "BW_DATABASE = os.environ.get('S2S_DB')\n",
    "BW_TABLE = os.environ.get('S2S_Table')\n",
    "BW_UID = os.environ.get('S2S_User')\n",
    "BW_PWD = os.environ.get('S2S_Pass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bef02-dbd7-4a29-af9c-208a39a78a87",
   "metadata": {},
   "source": [
    "Define and run a function for connecting to BeachWatch, querying all data records from BeachWatch, and returning the data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea66523-0939-4223-98a4-01fe9272cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date columns for both BeachWatch and CEDEN to ensure that date values get parsed correctly\n",
    "date_cols = ['SampleDate', 'CalibrationDate', 'CollectionTime', 'PrepPreservationDate', 'DigestExtractDate', 'AnalysisDate']\n",
    "\n",
    "def get_bw_data():\n",
    "    cnxn = pyodbc.connect(Driver='SQL Server', Server=BW_SERVER1, Database=BW_DATABASE, uid=BW_UID, pwd=BW_PWD)\n",
    "    sql =  \"SELECT * FROM %s\" % BW_TABLE\n",
    "    df = pd.read_sql_query(sql, cnxn, parse_dates=date_cols, dtype={'ResultReplicate': np.int16, 'CollectionReplicate': np.int16})\n",
    "    return df\n",
    "\n",
    "bw_df = get_bw_data() \n",
    "print(\"Count of rows:\", bw_df.shape[0])\n",
    "\n",
    "# Add a field for identifying the database source of the data\n",
    "bw_df['DataSource'] = 'BeachWatch'\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "bw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8449bc-d124-4fcf-a547-bc7551f85c99",
   "metadata": {},
   "source": [
    "Some of the BeachWatch columns have slightly different names compared to the CEDEN columns. Because we will be joining these two datasets, we want all of the column names to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b7308-5288-4d62-8811-c248bc49c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for mapping the names of BeachWatch fields to CEDEN fields\n",
    "bw_to_ceden_fields = {\n",
    "    'ProgramName': 'Program',\n",
    "    'ParentProjectName': 'ParentProject',\n",
    "    'ProjectName': 'Project',\n",
    "    'UnitName': 'Unit',\n",
    "    'ResQualCode': 'ResultQualCode',\n",
    "    'BatchVerificationCode': 'BatchVerification',\n",
    "    'LabCollectionComments': 'CollectionComments',\n",
    "    'LabResultComments': 'ResultsComments',\n",
    "    'AgencyCode': 'SampleAgency',\n",
    "    'CollectionDeviceName': 'CollectionDeviceDescription',\n",
    "    'LabSubmissionCode': 'SubmissionCode',\n",
    "    'ResultReplicate': 'ResultsReplicate'\n",
    "}\n",
    "\n",
    "bw_df = bw_df.rename(columns=bw_to_ceden_fields)\n",
    "bw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e50112-bb8f-410a-9411-105f1fee0e15",
   "metadata": {},
   "source": [
    "#### 2.2 CEDEN\n",
    "Define the variables for connecting to CEDEN. Like for the BeachWatch data above, these are private login credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695722fd-156c-4dca-abf4-98335865daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CEDEN_SERVER1 = os.environ.get('SERVER1')\n",
    "CEDEN_UID = os.environ.get('UID')\n",
    "CEDEN_PWD = os.environ.get('PWD')\n",
    "CEDEN_TABLE = os.environ.get('TABLE')\n",
    "CEDEN_SITE_DATUM_TABLE = os.environ.get('SITE_DATUM_TABLE') # Used for getting site datum data\n",
    "CEDEN_SITE_TABLE = os.environ.get('SITE_TABLE') # Used for getting site region number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417fa0af-cbe0-48a3-adbe-b6e661b564f0",
   "metadata": {},
   "source": [
    "Define and run a function for connecting to the CEDEN data mart and returning the data as a pandas dataframe. This query includes all data for E. coli, Enterococcus, Fecal Coliform, and Total Coliform, but at the same time it excludes all records where Program == BeachWatch. There is a lot of duplicate BeachWatch data in CEDEN from the time when BeachWatch data was copied over into CEDEN. We want to exclude the duplicate BeachWatch data from our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2ecce-c36a-4d7e-9994-8c09761b9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ceden_data():\n",
    "    cnxn = pyodbc.connect(Driver='SQL Server', Server=CEDEN_SERVER1, uid=CEDEN_UID, pwd=CEDEN_PWD)\n",
    "    sql = \"SELECT * FROM %s WHERE (Analyte in ('E. coli', 'Enterococcus', 'Coliform, Total', 'Coliform, Fecal') AND Program != 'BeachWatch')\" % CEDEN_TABLE\n",
    "    df = pd.read_sql_query(sql, cnxn, parse_dates=date_cols, dtype={'ResultsReplicate': np.int16, 'CollectionReplicate': np.int16})\n",
    "    return df\n",
    "\n",
    "ceden_df = get_ceden_data()\n",
    "print(\"Count of rows:\", ceden_df.shape[0])\n",
    "\n",
    "# Add data source field\n",
    "ceden_df['DataSource'] = 'CEDEN'\n",
    "\n",
    "ceden_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16e78a-0848-4ee0-ae6e-d03c344131c7",
   "metadata": {},
   "source": [
    "### 3. Combine the BeachWatch and CEDEN datasets\n",
    "The BeachWatch and CEDEN datasets have similar data structures, allowing us to combine the two datasets and work on both of them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e165b9e-b530-4b49-96bd-aaba9af03c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([bw_df, ceden_df],  ignore_index=True)\n",
    "print(\"Count of rows:\", combined_df.shape[0])\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b2578-38c6-4fe7-b495-0f0a715fbb13",
   "metadata": {},
   "source": [
    "### 4. Create the SampleDateTime column\n",
    "For CEDEN, the sample date and collection time are stored in two different columns, SampleDate and CollectionTime, respectively. CollectionTime has a recorded date along with a time, but the paired date is not usable. Create a new column by separating out the time value from the CollectionTime column and combine it with the date value in the SampleDate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20912779-0d5d-489c-999e-39a7aec88d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time value from CollectionTime field and copy to a new field\n",
    "combined_df['CollectionTimeOnly'] = combined_df['CollectionTime'].dt.time\n",
    "\n",
    "# Combine the date and time values into a new SampleDateTime field\n",
    "combined_df['SampleDateTime'] = pd.to_datetime(combined_df['SampleDate']) + pd.to_timedelta(combined_df['CollectionTimeOnly'].astype(str))\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e8909-c3e2-4230-ad68-aad6f4ebd862",
   "metadata": {},
   "source": [
    "### 5. Dropping duplicate records\n",
    "Even though we excluded BeachWatch records when pulling data from CEDEN (Step 2.2), there are still some duplicate BeachWatch records in CEDEN because these records are submitted to CEDEN under a different program name (i.e., not BeachWatch). \n",
    "\n",
    "An example of this is StationCode == 'Wharf-East' for Total coliform, sample taken on 9/5/2019. There are three data points for the same result, one in BeachWatch and two in CEDEN. They mostly have the same values in every column except for Program, ResultQualCode, and QACode. The Program value in BeachWatch is \"BeachWatch\" whereas the Program values in CEDEN are \"BeachWatch\" and \"Santa Cruz City Environmental Program\". The BeachWatch record was copied over into CEDEN from the BeachWatch database, and the other record was submitted to CEDEN under a different program name. Because the SQL query used in Step 2.2 only excludes records that have a Program value of \"BeachWatch\", the latter record would still make it into the combined dataset.\n",
    "\n",
    "A list of columns, defined below in the variable \"duplicate_cols\", is used to identify and drop the remaining duplicate records. When comparing one record to another, the code is looking for at least one unique value across all of these columns. If the values for both records across all columns are the same, then it is considered a duplicate record. This list of columns can be changed, as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de9f01-9d18-4ff9-a83a-d5e0ca7f703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by the DataSource column so that all BeachWatch records are positioned before the CEDEN records. \n",
    "# This is to ensure that BeachWatch records are kept by default if there happens to be the same record from both BeachWatch and CEDEN\n",
    "combined_df = combined_df.sort_values(by='DataSource')\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053690d5-4d44-4a29-977e-15830ccaf81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns used to identify duplicate records\n",
    "# 10/1/24 - I removed 'QACode' and 'ResultQualCode' from this list because it appears that some duplicate records across BeachWatch and CEDEN have different QACode and ResultQualCode values \n",
    "# See StationCode == 'Wharf-East' for Total coliform, samples taken on 9/5/2019 (QACode) and 9/23/2019 (ResultQualCode)\n",
    "duplicate_cols = ['Analyte', 'MatrixName', 'SampleDateTime', 'CollectionReplicate', 'ResultsReplicate', 'MethodName', 'Result', 'Unit']\n",
    "\n",
    "# Select the identified duplicate records from the combined dataset and copy them to a new dataframe\n",
    "# These records will later be added to the rejected_records csv file output\n",
    "duplicates_df = combined_df.loc[combined_df.duplicated(subset=duplicate_cols, keep='first')]\n",
    "duplicates_df['Comments'] = 'Duplicate record'\n",
    "\n",
    "print('Count of duplicate records:', duplicates_df.shape[0])\n",
    "duplicates_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da22c2a-3eae-4477-bba0-282aff25ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of rows before dropping duplicates:', combined_df.shape[0])\n",
    "\n",
    "# Drop the duplicate records from the combined dataset; if there are duplicates, keep the first duplicate record found (BeachWatch)\n",
    "combined_df = combined_df.drop_duplicates(subset=duplicate_cols, keep='first')\n",
    "\n",
    "print('Count of rows after removing duplicates:', combined_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3f973-e37e-413a-b4de-004d72278a63",
   "metadata": {},
   "source": [
    "### 6. Clean and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace60359-aea5-48d0-9e6e-0754f1f7523a",
   "metadata": {},
   "source": [
    "#### 6.1 Strip special characters and whitespace characters. Check null/missing values for compatability with the open data portal (data.ca.gov)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2965cc6-9682-468d-a195-6e3ba5e55d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip special characters. These characters can cause issues when reading, parsing, or writing the data\n",
    "combined_df.replace(r'\\t',' ', regex=True, inplace=True) # tab\n",
    "combined_df.replace(r'\\r',' ', regex=True, inplace=True) # carriage return\n",
    "combined_df.replace(r'\\n',' ', regex=True, inplace=True) # newline\n",
    "combined_df.replace(r'\\f',' ', regex=True, inplace=True) # formfeed\n",
    "combined_df.replace(r'\\v',' ', regex=True, inplace=True) # vertical tab\n",
    "combined_df.replace(r'\\|', ' ', regex=True, inplace=True) # pipe\n",
    "combined_df.replace(r'\\\"', ' ', regex=True, inplace=True) # quotes\n",
    "\n",
    "# Process the data to make sure the fields are compatible with the portalâ€™s data type definition. \n",
    "# For numeric, make sure that all values can be recognized as a number. Missing values have to be encoded as \"NaN\". \n",
    "# For dates, the data has to be formatted as YYYY-MM-DD (you can also add a time to that - YYYY-MM-DD HH:MM:SS), and missing values have to be encoded as an empty text string (\"\").\n",
    "# Check numeric columns\n",
    "\n",
    "numeric_cols = ['CollectionDepth', 'CollectionReplicate', 'ResultsReplicate', 'Result']\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        combined_df[col].fillna('NaN')\n",
    "    except:\n",
    "        print('%s field does not exist for dataframe' % col)\n",
    "\n",
    "# Cast data type for Result and MDL columns to numeric. Must be done here, not in the import data section\n",
    "combined_df['Result'] = pd.to_numeric(combined_df['Result'], errors='coerce')\n",
    "combined_df['MDL'] = pd.to_numeric(combined_df['MDL'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc7949-a2ed-4df1-a5a6-ba4211b7b253",
   "metadata": {},
   "source": [
    "#### 6.2 Check latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ebbc0-e5b6-4612-8e11-82f140cdc2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_latitude(val):\n",
    "    try:\n",
    "        lat = float(val)\n",
    "        return lat\n",
    "    except TypeError:\n",
    "        # a missing latitude value (and non-numeric values) should throw an error\n",
    "        # missing values should be encoded as 'NaN' to define data type as numeric on open data portal\n",
    "        return 'NaN'\n",
    "    except ValueError:\n",
    "        return 'NaN'\n",
    "\n",
    "# Sometimes the Longitude gets entered as 119 instead of -119...\n",
    "# Make sure Longitude value is negative and less than 10000 (could be projected)\n",
    "# Check for missing and non-numeric values, replace with 'NaN'\n",
    "def check_longitude(val):\n",
    "    try:\n",
    "        long = float(val)\n",
    "        if 0. < long < 10000.0:\n",
    "            val = -long\n",
    "        return val\n",
    "    except TypeError:\n",
    "        # a missing latitude value (and non-numeric values) should throw an error\n",
    "        # missing values should be encoded as 'NaN' to define data type as numeric on open data portal\n",
    "        return 'NaN'\n",
    "    except ValueError:\n",
    "        return 'NaN'\n",
    "\n",
    "combined_df['TargetLatitude'] = combined_df['TargetLatitude'].map(check_latitude).fillna('')\n",
    "combined_df['TargetLongitude'] = combined_df['TargetLongitude'].map(check_longitude).fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37c00b-11fd-446d-a2de-9f04dc9ca976",
   "metadata": {},
   "source": [
    "#### 6.3 Drop records that do not have valid Result and MDL values\n",
    "These records cannot be used even if we try to substitute the original value with 1/2 the MDL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f172358-a204-46bc-944d-2cd7649e315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy non-ND records that have a negative or null Result and a negative or null MDL value to a new dataframe\n",
    "# These records will later be added to the rejected_records csv file output\n",
    "rejected1_df = combined_df[((pd.isna(combined_df['Result'])) | (combined_df['Result'] < 0)) & ((pd.isna(combined_df['MDL'])) | (combined_df['MDL'] < 0)) & (combined_df['ResultQualCode'] != 'ND')]\n",
    "rejected1_df['Comments'] = 'Result is null or negative; MDL is null or negative'\n",
    "print('Count of unusable records to be dropped:', rejected1_df.shape[0])\n",
    "\n",
    "# Drop the records from the dataset\n",
    "combined_df = combined_df.drop(rejected1_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9457c48-e202-42a6-9af1-423b345d9ec0",
   "metadata": {},
   "source": [
    "#### 6.4 Drop replicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ba13c-e4f4-4cf8-97f3-8cf0993689dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy replicate records to a new dataframe\n",
    "# These records will later be added to the rejected_records csv file output\n",
    "replicate_df = combined_df[(combined_df['ResultsReplicate'] != 1) | (combined_df['CollectionReplicate'] != 1)]\n",
    "replicate_df['Comments'] = 'Replicate data'\n",
    "print('Count of replicate records to be dropped:', replicate_df.shape[0])\n",
    "\n",
    "combined_df = combined_df.drop(replicate_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94199b4d-1a75-4799-af0d-5a63e95145e3",
   "metadata": {},
   "source": [
    "#### 6.5 Standardize unit values and drop unneeded records\n",
    "There is inconsistency, mainly in the CEDEN database, with how the unit values are named. Later on, when calculating the geomeans, we will want to be able to group records by common unit values, so these values should match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593224ff-5d30-425a-bcf6-fd16bfc27310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename units with abbreviations to have all capitalized letters\n",
    "combined_df['Unit'] = combined_df['Unit'].replace('cfu/100mL', 'CFU/100 mL') \n",
    "combined_df['Unit'] = combined_df['Unit'].replace('mpn/100mL', 'MPN/100 mL') \n",
    "\n",
    "# Filter for specific units to be included in the dataset; copy all other records to new dataframe\n",
    "units_keep = ['MPN/100 mL', 'CFU/100 mL', 'copies/100 mL']\n",
    "rejected_units_df = combined_df[~combined_df['Unit'].isin(units_keep)]\n",
    "print('Count of unit records to filter out:', rejected_units_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56df12-3c22-47c9-9b58-847de6aec93a",
   "metadata": {},
   "source": [
    "#### 6.6 Categorize records into unit groups based on the unit name\n",
    "This is based on the assumption that results reported in MPN (most probable number) are equivalent to results reported in CFU (colony forming units). Result values reported in \"copies/100 mL\" are associated with ddPCR methods. They are not equivalent to either MPN/CFU and should be handled separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aba021-d23e-4cd8-a579-aa5e5466fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a numeric value to each record based on the UnitName value\n",
    "unit_map = { 'MPN/100 mL': 1, 'CFU/100 mL': 1, 'copies/100 mL': 2}\n",
    "combined_df['UnitGroup'] = combined_df['Unit'].map(unit_map)  \n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e5bce-824b-45f8-bcee-066223078b7a",
   "metadata": {},
   "source": [
    "### 7. Add Datum column to the dataset\n",
    "The data quality estimator tool (used in Step 8) requires the Datum field. This field is not included with the BeachWatch and CEDEN datasets by default, so we must get it from another CEDEN table and then join the values to the working dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef57b0f-cdbe-4e3f-b3fc-cc8eb31b2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function used to get all records from the CEDEN table with datum data\n",
    "def get_datum_data():\n",
    "    try:\n",
    "        sql = \"SELECT StationCode, Datum FROM %s ;\" % CEDEN_SITE_DATUM_TABLE\n",
    "        cnxn = pyodbc.connect(Driver='SQL Server', Server=CEDEN_SERVER1, uid=CEDEN_UID, pwd=CEDEN_PWD)\n",
    "        df = pd.read_sql(sql, cnxn)\n",
    "        return df\n",
    "    except:\n",
    "        print(\"Couldn't connect to %s.\" % CEDEN_SERVER1)\n",
    "\n",
    "datum_df = get_datum_data()\n",
    "datum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52016e11-fbe5-4ab8-a818-b6f74ecec1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the datum data to the combined dataset on common StationCode IDs\n",
    "data_df = pd.merge(combined_df, datum_df, on='StationCode', how='left')\n",
    "\n",
    "# Fill empty datum values with 'NR'. This is an important step for the data quality estimator, used later\n",
    "data_df = data_df.fillna(value={'Datum': 'NR'})\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd884ed3-7900-40ed-87c7-8d589a5617a9",
   "metadata": {},
   "source": [
    "### 8. Add a RegionNumber column to the dataset\n",
    "This is a requested column to identify the Regional Board area where the site is located. We have to get data from another CEDEN stations table and join it to this dataset. This CEDEN table is a different table than the one used in Step 7. Unfortunately, the RB number values from this table are not complete. There will be some null values and other non-standard values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf8b94-68d6-4458-920f-011f5623eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that gets all records from the CEDEN station table, used to join region values.\n",
    "def get_ceden_site_data():\n",
    "    cnxn = pyodbc.connect(Driver='SQL Server', Server=CEDEN_SERVER1, uid=CEDEN_UID, pwd=CEDEN_PWD)\n",
    "    sql = \"SELECT StationLUCode, rb_number FROM %s\" % CEDEN_SITE_TABLE\n",
    "    df = pd.read_sql_query(sql, cnxn)\n",
    "    return df\n",
    "\n",
    "site_data = get_ceden_site_data()\n",
    "site_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e59461-c895-420f-87bd-92d1271cbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the Region number to the combined dataset\n",
    "data_df = data_df.merge(site_data, how='left', left_on='StationCode', right_on='StationLUCode')\n",
    "data_df = data_df.rename(columns={'rb_number': 'RegionNumber'})\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540feb1b-7078-4cf7-ab39-650b80628a51",
   "metadata": {},
   "source": [
    "### 9. Add data quality columns to the dataset\n",
    "The OIMA data quality estimator tool adds two columns, DataQuality and DataQualityIndicator, to the dataset.\n",
    "\n",
    "DataQuality: Describes the overall quality of the record by taking the QACode, ResulualQACode, ComplicanceCode, BatchVerificationCode, and special circumstances into account to assign it to one of the following categories: Passed, Some review needed, Spatial accuracy unknown, Extensive review needed, Unknown data quality, Reject record, Error in data, Metadata. The assignments and categories are provisional. A working explanation of the data quality ranking can be found this Google Doc file: https://docs.google.com/spreadsheets/d/1q-tGulvO9jyT2dR9GGROdy89z3W6xulYaci5-ezWAe0/edit?usp=sharing\n",
    "\n",
    "DataQualityIndicator - Explains the reason for the DataQuality value by indicating which quality assurance check the data did not pass (e.g. BatchVerificationCode, ResultQACode, etc.).\n",
    "\n",
    "The function \"add_data_quality\" used to add these two columns is imported into this notebook from another Python script file (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24666b-e49c-4884-8394-bade3e9e77b1",
   "metadata": {},
   "source": [
    "The code for the data quality estimator is hosted on GitHub here: https://github.com/mmtang/data-quality-estimator.\n",
    "- The function *add_data_quality*: https://github.com/mmtang/data-quality-estimator/blob/master/data_quality.py\n",
    "- The dictionaries for QACodes, ResultQualCodes, ComplianceCodes, etc. and their associated data quality values: https://github.com/mmtang/data-quality-estimator/blob/master/dq_constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b1358-0e05-4117-a440-cdff362ca4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python file with the data quality estimator functions\n",
    "import sys\n",
    "sys.path.append('../data-quality-estimator')  # Path contains data_quality_utils.py\n",
    "\n",
    "import data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34695a71-b02e-4099-8557-88154137dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the DataQuality and DataQualityIndicator columns\n",
    "data_df = data_quality.add_data_quality(data_df, 'chemistry')\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89adf5fe-efb7-4f45-ad68-e6b5f2eecef3",
   "metadata": {},
   "source": [
    "### 10. Drop records with a DataQuality score of \"Reject record\" or \"Metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337de787-0ef3-4ad3-8e19-4ba9958fb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy records with a DataQuality score of 'Reject record' or 'MetaData to a new dataframe\n",
    "# These records will later be added to the rejected_records csv file output\n",
    "dq_filter = ['Reject record', 'MetaData']\n",
    "reject_dq_df = data_df[data_df['DataQuality'].isin(dq_filter)]\n",
    "\n",
    "# Drop these records from the dataset\n",
    "data_df = data_df[~data_df['DataQuality'].isin(dq_filter)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3e1af-a1dc-4803-8d9a-34de71bea488",
   "metadata": {},
   "source": [
    "### 11. Clean null values\n",
    "For compatability with the open data portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e71528-e5af-4531-a2fb-bef3cd31df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to make a distinction between None, 'None', and ''\n",
    "# 'None' and '' are used specifically in the datasets, but None gets translated to 'None' unless we replace it with '' explicitly\n",
    "data_df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e296552-91cf-4f17-9ad5-5adeae6fd61a",
   "metadata": {},
   "source": [
    "### 12. Export a CSV file of all the dropped records. This includes:\n",
    "\n",
    "- Step 5: Dropped duplicate records (duplicates_df)\n",
    "- Step 6.3: Dropped records with unusable Result and MDL values (rejected1_df)\n",
    "- Step 6.4: Dropped replicate records (replicate_df)\n",
    "- Step 6.5: Dropped records with unit values we are not using (rejected_units_df)\n",
    "- Step 10: Dropped data quality records (reject_dq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebe76e-1fe9-4f1e-aa75-d8af6fe099c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fields to be included in file export\n",
    "reject_export_fields = [\n",
    "    'Program',\n",
    "    'ParentProject',\n",
    "    'Project',\n",
    "    'StationName',\n",
    "    'StationCode',\n",
    "    'SampleDate',\n",
    "    'CollectionTime',\n",
    "    'LocationCode',\n",
    "    'CollectionDepth',\n",
    "    'UnitCollectionDepth',\n",
    "    'SampleTypeCode',\n",
    "    'CollectionReplicate',\n",
    "    'ResultsReplicate',\n",
    "    'LabBatch',\n",
    "    'LabSampleID',\n",
    "    'MatrixName',\n",
    "    'MethodName',\n",
    "    'Analyte',\n",
    "    'Unit',\n",
    "    'Result',\n",
    "    'Observation',\n",
    "    'MDL',\n",
    "    'RL',\n",
    "    'ResultQualCode',\n",
    "    'QACode',\n",
    "    'BatchVerification',\n",
    "    'ComplianceCode',\n",
    "    'SampleComments',\n",
    "    'CollectionComments',\n",
    "    'ResultsComments',\n",
    "    'BatchComments',\n",
    "    'EventCode',\n",
    "    'ProtocolCode',\n",
    "    'SampleAgency',\n",
    "    'GroupSamples',\n",
    "    'CollectionMethodName',\n",
    "    'TargetLatitude',\n",
    "    'TargetLongitude',\n",
    "    'CollectionDeviceDescription',\n",
    "    'CalibrationDate',\n",
    "    'PositionWaterColumn',\n",
    "    'PrepPreservationName',\n",
    "    'PrepPreservationDate',\n",
    "    'DigestExtractMethod',\n",
    "    'DigestExtractDate',\n",
    "    'AnalysisDate',\n",
    "    'DilutionFactor',\n",
    "    'ExpectedValue',\n",
    "    'LabAgency',\n",
    "    'SubmittingAgency',\n",
    "    'SubmissionCode',\n",
    "    'OccupationMethod',\n",
    "    'StartingBank',\n",
    "    'DistanceFromBank',\n",
    "    'UnitDistanceFromBank',\n",
    "    'StreamWidth',\n",
    "    'UnitStreamWidth',\n",
    "    'StationWaterDepth',\n",
    "    'UnitStationWaterDepth',\n",
    "    'HydroMod',\n",
    "    'HydroModLoc',\n",
    "    'LocationDetailWQComments',\n",
    "    'ChannelWidth',\n",
    "    'UpstreamLength',\n",
    "    'DownStreamLength',\n",
    "    'TotalReach',\n",
    "    'LocationDetailBAComments',\n",
    "    'SampleID',\n",
    "    'DW_AnalyteName',\n",
    "    'UnitGroup',\n",
    "    'Datum',\n",
    "    'DataSource',\n",
    "    'SampleDateTime',\n",
    "    'RegionNumber',\n",
    "    'DataQuality',\n",
    "    'DataQualityIndicator',\n",
    "    'Comments'\n",
    "]\n",
    "\n",
    "# Merge all dataframes into a single dataframe\n",
    "all_dropped_records_df = pd.concat([duplicates_df, rejected1_df, replicate_df, rejected_units_df, reject_dq_df], ignore_index=True)\n",
    "all_dropped_records_df = all_dropped_records_df[reject_export_fields]\n",
    "\n",
    "all_dropped_records_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1250b-0bc0-4ab4-bef2-61e0f7111aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all rejected records as a CSV file\n",
    "all_dropped_records_df.to_csv('SafeToSwim_rejected_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867ee18-0e06-4606-b53b-b8d6f6c0f3af",
   "metadata": {},
   "source": [
    "### 13. Handle non-detect (ND) records and assign substitute Result values\n",
    "If a record is flagged as non-detect (ResultQualCode == 'ND'), substitute the Result value with either half the original Result value (if the Result > 0) or half the MDL (if the Result <= 0 or Result is null).\n",
    "\n",
    "Also substitute half the MDL for records that are not flagged as non-detect but for some reason have a zero, null, or negative Result value. There shouldn't be very many (if any) of these records at this point, but I've left the code here just in case any slip through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2deec3-1522-4334-b88c-ec22a0739afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for assigning substitute Result values\n",
    "def subResult(row):\n",
    "    if (row['ResultQualCode'] == 'ND'):\n",
    "        if (row['Result'] > 0):\n",
    "            return pd.Series([(0.5 * row['Result']), 'Nondetect: result substituted with half the result value'])\n",
    "        elif (row['MDL'] > 0):\n",
    "            return pd.Series([(0.5 * row['MDL']), 'Nondetect: result substituted with half the MDL'])\n",
    "        else:\n",
    "            return pd.Series([row['Result'], 'No substitution'])\n",
    "    elif ((row['Result'] == 0) or (pd.isna(row['Result'])) or (row['Result'] < 0)):\n",
    "        if (row['MDL'] > 0):\n",
    "            return pd.Series([(0.5 * row['MDL']), 'Result substituted with half the MDL'])\n",
    "        else:\n",
    "            return pd.Series([row['Result'], 'No substitution'])\n",
    "    else:\n",
    "        return pd.Series([row['Result'], 'No substitution'])\n",
    "\n",
    "# Apply the function to the entire dataframe and save the subbed and non-subbed Result values to a new dataframe\n",
    "sub_values = data_df.apply(lambda x: subResult(x), axis=1)\n",
    "\n",
    "# Copy over the values and comments to the original dataframe as a new column \"ResultSub\". The original \"Result\" column is left untouched for reference.\n",
    "data_df['ResultSub'], data_df['ResultSubComments'] = sub_values[0], sub_values[1]\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22a819-fadb-4e2b-b503-eecbc7b67ff1",
   "metadata": {},
   "source": [
    "### 14. Calculate the geometric mean values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae9723-f181-4fb1-8b82-2862e02e1321",
   "metadata": {},
   "source": [
    "#### 14.1 Required data prep before calculating the geometric mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527d4fc-7bdb-4a1e-aee0-48cc14dba425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that SampleDateTime values are cast as datetime objects\n",
    "data_df['SampleDateTime'] = data_df['SampleDateTime'].astype('datetime64[ns]')\n",
    "\n",
    "# Set SampleDateTime as the index. This is more efficient for the grouping operations\n",
    "data_df.set_index('SampleDateTime', inplace=True) \n",
    "\n",
    "# Drop records that have a null/NaT SampleDate value. As of 6-18-24, this is just one record.\n",
    "data_df = data_df.loc[data_df.index.notnull()] \n",
    "\n",
    "# Sort records based on ascending SampleDateTime. A bit counterintuitive, but this is the setup for calculating \n",
    "# the rolling geometric starting from the most recent sample date working backwards using the rolling function\n",
    "data_df.sort_index(ascending=True, inplace=True) \n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97934c20-ffa0-4866-b69f-30bf88f637be",
   "metadata": {},
   "source": [
    "#### 14.2 Group records and calculate the geometric mean\n",
    "This code block adds four new columns:\n",
    "\n",
    "- 30DayGeoMean: The rolling geometric mean value looking back 30 days from the recorded sample date.\n",
    "- 30DayCount: The number of distinct sample result values included in the 30 day date range and used in the geometric mean calculation.\n",
    "- 6WeekGeoMean: The rolling geometric mean value looking back 6 weeks (42 days) from the recorded sample date.\n",
    "- 6WeekCount: The number of distinct sample result values included in the 6 week date range and used in the geometric mean calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d2107-57a0-48ed-9a62-94b6a4791826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating and adding the geometric mean columns to a grouped dataframe\n",
    "def process_group(df):\n",
    "    # Nested function for calculating the geometric mean         \n",
    "    def calculateGeometricMean(x):\n",
    "        # Need to group records again or else the Result values are duplicated in the calculation\n",
    "        x = x.groupby(level=0).mean()\n",
    "        g_value = gmean(x, nan_policy='omit') # gmean is a SciPy function\n",
    "        return g_value\n",
    "\n",
    "    # It is not recommended to mutate the object we're iterating on, thus the copy:\n",
    "    # https://pandas.pydata.org/docs/user_guide/gotchas.html#mutating-with-user-defined-function-udf-methods\n",
    "    df = df.copy() \n",
    "\n",
    "    # Calculate 30 day rolling geomean\n",
    "    df['30DayGeoMean'] = df['ResultSub'].rolling(window='30D', min_periods=1, closed='both').apply(calculateGeometricMean).round(3) \n",
    "    df['30DayCount'] = df['ResultSub'].rolling(window='30D', min_periods=1, closed='both').apply(lambda x: len(x.groupby(level=0)))\n",
    "\n",
    "    # Calculate 6 week (42 days) rolling geomean\n",
    "    df['6WeekGeoMean'] = df['ResultSub'].rolling(window='42D', min_periods=1, closed='both').apply(calculateGeometricMean).round(3)\n",
    "    df['6WeekCount'] = df['ResultSub'].rolling(window='42D', min_periods=1, closed='both').apply(lambda x: len(x.groupby(level=0))) \n",
    "\n",
    "    # Drop duplicate records\n",
    "    df = df.groupby(level=0).last()\n",
    "    return df\n",
    "\n",
    "# Calculate new geometric mean values for all FIB records based on the SampleDateTime index and common column values as defined in group_cols\n",
    "# Set allow_duplicates=True to reinsert index columns into the dataframe and allow columns with the same name\n",
    "group_cols = ['Analyte', 'StationCode', 'UnitGroup']\n",
    "grouped_df = data_df.groupby(group_cols).apply(process_group).reset_index(allow_duplicates=True)\n",
    "\n",
    "# Drop duplicate columns. There might be some duplicate columns leftover after the new geomean columns are inserted back into the dataframe\n",
    "grouped_df = grouped_df.loc[:,~grouped_df.columns.duplicated()]\n",
    "\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31063865-a7a7-465f-bc91-dfacca00f279",
   "metadata": {},
   "source": [
    "### 15. Export the geomean dataset as a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036383c-57bb-4a0f-8849-4b4550b7568d",
   "metadata": {},
   "source": [
    "#### 15.1 Export the dataset with all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03d729-52e4-4e8f-bca9-bacaa36b9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields = [\n",
    "    'Program',\n",
    "    'ParentProject',\n",
    "    'Project',\n",
    "    'StationName',\n",
    "    'StationCode',\n",
    "    'SampleDate',\n",
    "    'CollectionTime',\n",
    "    'LocationCode',\n",
    "    'CollectionDepth',\n",
    "    'UnitCollectionDepth',\n",
    "    'SampleTypeCode',\n",
    "    'CollectionReplicate',\n",
    "    'ResultsReplicate',\n",
    "    'LabBatch',\n",
    "    'LabSampleID',\n",
    "    'MatrixName',\n",
    "    'MethodName',\n",
    "    'Analyte',\n",
    "    'Unit',\n",
    "    'Result',\n",
    "    'Observation',\n",
    "    'MDL',\n",
    "    'RL',\n",
    "    'ResultQualCode',\n",
    "    'QACode',\n",
    "    'BatchVerification',\n",
    "    'ComplianceCode',\n",
    "    'SampleComments',\n",
    "    'CollectionComments',\n",
    "    'ResultsComments',\n",
    "    'BatchComments',\n",
    "    'EventCode',\n",
    "    'ProtocolCode',\n",
    "    'SampleAgency',\n",
    "    'GroupSamples',\n",
    "    'CollectionMethodName',\n",
    "    'TargetLatitude',\n",
    "    'TargetLongitude',\n",
    "    'CollectionDeviceDescription',\n",
    "    'CalibrationDate',\n",
    "    'PositionWaterColumn',\n",
    "    'PrepPreservationName',\n",
    "    'PrepPreservationDate',\n",
    "    'DigestExtractMethod',\n",
    "    'DigestExtractDate',\n",
    "    'AnalysisDate',\n",
    "    'DilutionFactor',\n",
    "    'ExpectedValue',\n",
    "    'LabAgency',\n",
    "    'SubmittingAgency',\n",
    "    'SubmissionCode',\n",
    "    'OccupationMethod',\n",
    "    'StartingBank',\n",
    "    'DistanceFromBank',\n",
    "    'UnitDistanceFromBank',\n",
    "    'StreamWidth',\n",
    "    'UnitStreamWidth',\n",
    "    'StationWaterDepth',\n",
    "    'UnitStationWaterDepth',\n",
    "    'HydroMod',\n",
    "    'HydroModLoc',\n",
    "    'LocationDetailWQComments',\n",
    "    'ChannelWidth',\n",
    "    'UpstreamLength',\n",
    "    'DownStreamLength',\n",
    "    'TotalReach',\n",
    "    'LocationDetailBAComments',\n",
    "    'SampleID',\n",
    "    'DW_AnalyteName',\n",
    "    #'UnitGroup',\n",
    "    'Datum',\n",
    "    #'CollectionTimeOnly',\n",
    "    'DataSource',\n",
    "    'SampleDateTime',\n",
    "    'RegionNumber',\n",
    "    'DataQuality',\n",
    "    'DataQualityIndicator',\n",
    "    'ResultSub',\n",
    "    'ResultSubComments',\n",
    "    #'ResultAvg',\n",
    "    '30DayGeoMean',\n",
    "    '30DayCount',\n",
    "    '6WeekGeoMean',\n",
    "    '6WeekCount'\n",
    "]\n",
    "\n",
    "# Order columns\n",
    "grouped_df_full = grouped_df[all_fields]\n",
    "\n",
    "# Export dataframe as a CSV file\n",
    "grouped_df_full.to_csv('SafeToSwim_geomeans.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d2247-14d6-4068-b5b7-1f54fa5400dd",
   "metadata": {},
   "source": [
    "#### 15.2 Dataset with select columns (for testing)\n",
    "Expprt an shortened version of the dataset (fewer columns) for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47d50b-7933-4b71-992c-2363a886c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fields = [\n",
    "    'Program',\n",
    "    'ParentProject',\n",
    "    'Project',\n",
    "    'StationName',\n",
    "    'StationCode',\n",
    "    'SampleDate',\n",
    "    'CollectionTime',\n",
    "    #'LocationCode',\n",
    "    #'CollectionDepth',\n",
    "    #'UnitCollectionDepth',\n",
    "    #'SampleTypeCode',\n",
    "    #'CollectionReplicate',\n",
    "    #'ResultsReplicate',\n",
    "    'LabBatch',\n",
    "    #'LabSampleID',\n",
    "    'MatrixName',\n",
    "    'MethodName',\n",
    "    'Analyte',\n",
    "    'Unit',\n",
    "    'Result',\n",
    "    #'Observation',\n",
    "    'MDL',\n",
    "    'RL',\n",
    "    'ResultQualCode',\n",
    "    #'QACode',\n",
    "    #'BatchVerification',\n",
    "    #'ComplianceCode',\n",
    "    #'SampleComments',\n",
    "    #'CollectionComments',\n",
    "    #'ResultsComments',\n",
    "    #'BatchComments',\n",
    "    #'EventCode',\n",
    "    #'ProtocolCode',\n",
    "    #'SampleAgency',\n",
    "    #'GroupSamples',\n",
    "    #'CollectionMethodName',\n",
    "    #'TargetLatitude',\n",
    "    #'TargetLongitude',\n",
    "    #'CollectionDeviceDescription',\n",
    "    #'CalibrationDate',\n",
    "    #'PositionWaterColumn',\n",
    "    #'PrepPreservationName',\n",
    "    #'PrepPreservationDate',\n",
    "    #'DigestExtractMethod',\n",
    "    #'DigestExtractDate',\n",
    "    #'AnalysisDate',\n",
    "    #'DilutionFactor',\n",
    "    #'ExpectedValue',\n",
    "    #'LabAgency',\n",
    "    #'SubmittingAgency',\n",
    "    #'SubmissionCode',\n",
    "    #'OccupationMethod',\n",
    "    #'StartingBank',\n",
    "    #'DistanceFromBank',\n",
    "    #'UnitDistanceFromBank',\n",
    "    #'StreamWidth',\n",
    "    #'UnitStreamWidth',\n",
    "    #'StationWaterDepth',\n",
    "    #'UnitStationWaterDepth',\n",
    "    #'HydroMod',\n",
    "    #'HydroModLoc',\n",
    "    #'LocationDetailWQComments',\n",
    "    #'ChannelWidth',\n",
    "    #'UpstreamLength',\n",
    "    #'DownStreamLength',\n",
    "    #'TotalReach',\n",
    "    #'LocationDetailBAComments',\n",
    "    #'SampleID',\n",
    "    #'DW_AnalyteName',\n",
    "    #'Datum',\n",
    "    #'CollectionTimeOnly',\n",
    "    'DataSource',\n",
    "    'SampleDateTime',\n",
    "    'RegionNumber',\n",
    "    'DataQuality',\n",
    "    'DataQualityIndicator',\n",
    "    'ResultSub',\n",
    "    'ResultSubComments',\n",
    "    #'ResultAvg',\n",
    "    '30DayGeoMean',\n",
    "    '30DayCount',\n",
    "    '6WeekGeoMean',\n",
    "    '6WeekCount'\n",
    "]\n",
    "\n",
    "# Order columns\n",
    "grouped_df_test = grouped_df[test_fields]\n",
    "\n",
    "# Export dataframe as a CSV file\n",
    "grouped_df_test.to_csv('SafeToSwim_geomeans_short.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
